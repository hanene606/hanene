# -*- coding: utf-8 -*-
"""ClassificationWithCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A_k4sqEE-dTB__uGzFBX_4kfsOvtHd5Z
"""

from random import shuffle, choice
from PIL import Image
import os
import numpy as np
import matplotlib as plt
import random
import glob
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import time
import pandas as pd
import cv2
import math
from random import randint



def load_rgb_data(IMAGE_DIRECTORY,IMAGE_SIZE, shuffle=True):
    print("Loading images...")
    data = []
    #labels=[]
    directories = next(os.walk(IMAGE_DIRECTORY))[1]
    print(directories)
    for diretcory_name in directories:
        print("Loading {0}".format(diretcory_name))
        file_names = next(os.walk(os.path.join(IMAGE_DIRECTORY, diretcory_name)))[2]
        print("we will load [", len(file_names), "] files from [",diretcory_name,"] class ..." )
        for i in range(len(file_names)):

          image_name = file_names[i]
          image_path = os.path.join(IMAGE_DIRECTORY, diretcory_name, image_name)
          if ('.DS_Store' not in image_path):
            #print(image_path)
            label = diretcory_name
            img = Image.open(image_path)
            rgbimg = Image.new("RGB", img.size)
            rgbimg.paste(img)
            img=rgbimg
            
            #print(np.array(img).shape)
            img = img.resize((IMAGE_SIZE, IMAGE_SIZE), Image.ANTIALIAS)
            #print(np.array(img).shape)
            data.append([np.array(img), label])

    if (shuffle):
      random.shuffle(data)
    training_images = np.array([i[0] for i in data]).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 3)
    training_labels = np.array([i[1] for i in data])
    
    print("File loading completed.")

    return training_images, training_labels


def load_rgb_data_cv(IMAGE_DIRECTORY,IMAGE_SIZE, shuffle=True):
    print("Loading images...")
    data = []
    labels=[]
    directories = next(os.walk(IMAGE_DIRECTORY))[1]

    for diretcory_name in directories:
        print("Loading {0}".format(diretcory_name))
        file_names = next(os.walk(os.path.join(IMAGE_DIRECTORY, diretcory_name)))[2]
        print("we will load [", len(file_names), "] files from [",diretcory_name,"] class ..." )
        for i in range(len(file_names)):
          image_name = file_names[i]
          image_path = os.path.join(IMAGE_DIRECTORY, diretcory_name, image_name)
          #print(image_path)
          label = diretcory_name

          img = cv2.imread(image_path)
          #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
          #img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))


          #print(np.array(img).shape)
          data.append([np.array(img), label])

    if (shuffle):
      random.shuffle(data)
    training_images = np.array([i[0] for i in data])
    training_labels = np.array([i[1] for i in data])
    
    print("File loading completed.")

    return training_images, training_labels


def normalize_data(dataset):
  print("normalize data")
  dataset= dataset/255.0
  return dataset
     


def display_image(trainX, trainY, index=0):
  plt.imshow(trainX[index])
  print ("Label = " + str(np.squeeze(trainY[index])))
  print ("image shape: ",trainX[index].shape)

def display_one_image(one_image, its_label):
  plt.imshow(one_image)
  print ("Label = " + its_label)
  print ("image shape: ",one_image.shape)


def display_dataset_shape(X,Y):
  print("Shape of images: ", X.shape)
  print("Shape of labels: ", Y.shape)
  

def plot_sample_from_dataset(images, labels,rows=5, colums=5, width=8,height=8):

  plt.figure(figsize=(width,height))
  for i in range(rows*colums):
      plt.subplot(rows,colums,i+1)
      plt.xticks([])
      plt.yticks([])
      plt.grid(False)
      plt.imshow(images[i], cmap=plt.cm.binary)
      plt.xlabel(labels[i])
  plt.show()

def display_dataset_folders(path):
  classes=os.listdir(path)
  classes.sort()
  print(classes)
  

def get_data_distribution(IMAGE_DIRECTORY, output_file,plot_stats=True):
    print("Loading images...")
    #list structure to collect the statistics
    stats=[]

    #get all image directories
    directories = next(os.walk(IMAGE_DIRECTORY))[1]

    for diretcory_name in directories:
        print("Loading {0}".format(diretcory_name))
        images_file_names = next(os.walk(os.path.join(IMAGE_DIRECTORY, diretcory_name)))[2]
        print("we will load [", len(images_file_names), "] files from [",diretcory_name,"] class ..." )
        for i in range(len(images_file_names)):
          image_name = images_file_names[i]
          image_path = os.path.join(IMAGE_DIRECTORY, diretcory_name, image_name)
          #print(image_path)

          #the class is assumed to be equal to the directorty name
          label = diretcory_name 

          img = Image.open(image_path)
          #convert any image to RGB to make sure that it has three channels
          rgbimg = Image.new("RGB", img.size)
          rgbimg.paste(img)
          img=rgbimg
          
          #get the width and the height of the image in pixels
          width,height = img.size
          #get the size of the image in KB
          size_kb=os.stat(image_path).st_size/1000
          #add the size to a list of sizes to be 
          stats.append([label,os.path.basename(image_name),width,height,size_kb])

    if (output_file is not None):
      #convert the list into a dataframe to make it easy to save into a CSV
      stats_dataframe = pd.DataFrame(stats,columns=['Class','Filename','Width','Height','Size_in_KB'])
      stats_dataframe.to_csv(output_file,index=False)
      print("Stats collected and saved in .",output_file)
    else:
      print("Stats collected");


    return stats


def plot_dataset_distribution (stats, num_cols=5, width=10, height=5, histogram_bins = 10, histogram_range=[0, 1000], figure_padding=4):
  #convert the list into a dataframe
  stats_frame = pd.DataFrame(stats,columns=['Class','Filename','Width','Height','Size_in_KB'])

  #extract the datframe related to sizes only
  list_sizes=stats_frame['Size_in_KB']

  #get the number of classes in the dataset
  number_of_classes=stats_frame['Class'].nunique()
  print(number_of_classes, " classes found in the dataset")

  #create a list of (list of sizes) for each class of images
  #we start by the the sizes of all images in the dataset
  list_sizes_per_class=[list_sizes] 
  class_names=['whole dataset']
  print("Images of the whole dataset have an average size of ", list_sizes.mean())
  
  for c in stats_frame['Class'].unique():
    print("sizes of class [", c, "] have an average size of ", list_sizes.loc[stats_frame['Class']== c].mean())
    #then, we append the sizes of images of a particular class
    list_sizes_per_class.append(list_sizes.loc[stats_frame['Class'] == c])
    class_names.append(c)

  num_rows=math.ceil((number_of_classes+1)/num_cols)
  if (number_of_classes<num_cols):
    num_cols=number_of_classes+1
  fig,axes = plt.subplots(num_rows, num_cols, figsize=(width,height))
    

  fig.tight_layout(pad=figure_padding)
  class_count=0
  if (num_rows==1 or num_cols==1):
    for i in range(num_rows):
      for j in range(num_cols): 
        axes[j+i].hist(list_sizes_per_class[num_cols*i+j], bins = histogram_bins, range=histogram_range)
        axes[j+i].set_xlabel('Image size (in KB)', fontweight='bold')
        axes[i+j].set_title(class_names[j+i] + ' images ', fontweight='bold')
        class_count=class_count+1
        if (class_count==number_of_classes+1):
          break
  
  else:
    for i in range(num_rows):
      for j in range(num_cols): 
        axes[i,j].hist(list_sizes_per_class[num_cols*i+j], bins = histogram_bins, range=histogram_range)
        axes[i,j].set_xlabel('Image size (in KB)', fontweight='bold')
        axes[i,j].set_title(class_names[i] + ' images ', fontweight='bold')
        class_count=class_count+1
        if (class_count==number_of_classes+1):
          break


def reshape_image_for_neural_network_input(image, IMAGE_SIZE=224):
    print ("flatten the image")
    image = np.reshape(image,[IMAGE_SIZE* IMAGE_SIZE*3,1])
    print ("image.shape", image.shape)
    print ("reshape the image to be similar to the input feature vector")
    #image = np.reshape(image,[1,IMAGE_SIZE, IMAGE_SIZE,3]).astype('float')
    image = image.reshape(1,IMAGE_SIZE,IMAGE_SIZE,3).astype('float')
    print ("image.shape", image.shape)
    return image

def plot_loss_accuracy(H, EPOCHS, output_file=None):  
  N = EPOCHS
  plt.style.use("ggplot")
  plt.figure()
  plt.plot(np.arange(0, N), H.history["loss"], label="train_loss")
  plt.plot(np.arange(0, N), H.history["val_loss"], label="val_loss")
  plt.plot(np.arange(0, N), H.history["accuracy"], label="train_acc")
  plt.plot(np.arange(0, N), H.history["val_accuracy"], label="val_acc")
  plt.title("Training Loss and Accuracy on COVID-19 Dataset")
  plt.xlabel("Epoch #")
  plt.ylabel("Loss/Accuracy")
  plt.legend(loc="lower left")
  if (output_file is not None):
    plt.savefig(output_file)



def draw_accuracy_graph(history):
    # summarize history for accuracy
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()
    
def draw_loss_graph(history):
    # summarize history for loss
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()


#-------------------------------#

def plot_test_image(testX, image_index, predictions_array, true_binary_labels):
    """
        testX: this is the test dataset
        image_index: index of the image that we will plot from the test dataset
        predictions_array: it is the array that contains all the predictions of the test dataset as output of model.predict(testX)
        true_binary_labels: these are the true label expressed as INTEGER values. It does not work with hot-encoding and string labels. 
    """
    single_predictions_array, true_binary_label, test_image = predictions_array, true_binary_labels[image_index], testX[image_index]
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])

    plt.imshow(test_image, cmap=plt.cm.binary)

    predicted_binary_label = np.argmax(predictions_array)
    #print ("predicted_binary_label:", predicted_binary_label)
    #print ("true_binary_label:",true_binary_label)
    
    if predicted_binary_label == true_binary_label:
        color = 'blue'
    else:
        color = 'red'

    plt.xlabel("predicted: {} {:2.0f}% (true: {})".format(predicted_binary_label,
                                100*np.max(single_predictions_array),
                                true_binary_label),
                                color=color)

def plot_value_array(i, predictions_array, true_label, number_of_classes=3):
    predictions_array, true_label = predictions_array, true_label[i]
    plt.style.use(['classic'])
    plt.grid(False)
    plt.xticks(range(number_of_classes))
    plt.yticks([])
    thisplot = plt.bar(range(number_of_classes), 1, color="#FFFFFF")
    plt.ylim([0, 1])
    predicted_label = np.argmax(predictions_array)
    #print(true_label[0])
    #print(predicted_label)
    
    thisplot[predicted_label].set_color('red')
    thisplot[true_label[0]].set_color('blue')



def plot_sample_predictions(testX, predictions_array, true_binary_labels,number_of_classes=3,num_rows = 10, num_cols = 4, width=None, height=None, is_random=True):
    """
        this method plots a sample of predictions from the test dataset and highlight correct and wrong predictions
        testX: this is the test dataset
        image_index: index of the image that we will plot from the test dataset
        predictions_array: it is the array that contains all the predictions of the test dataset as output of model.predict(testX)
        true_binary_labels: these are the true labels array expressed as INTEGER values. It does not work with hot-encoding and string labels. 
    """
    num_images = num_rows*num_cols

    if (num_images>testX.shape[0]):
      raise Exception("num_rows*num_cols is",(num_rows*num_cols), "must be smaller than number of images in the Test Dataset", testX.shape[0])

    if width is None:
      width=6*num_cols

    if height is None:
      height=2*num_rows

    plt.figure(figsize=(width, height))
    plt.style.use(['seaborn-bright'])
    
    image_index=-1
    for i in range(num_images):
        if (is_random==True):
          image_index=randint(0,testX.shape[0]-1)
        else:
          image_index=image_index+1

        #print(image_index)
        #---------------
        plt.subplot(num_rows, 2*num_cols, 2*i+1)
        plot_test_image(testX, image_index, predictions_array[image_index], true_binary_labels)
        #---------------
        plt.subplot(num_rows, 2*num_cols, 2*i+2)
        plot_value_array(image_index, predictions_array[image_index], true_binary_labels, number_of_classes)
    plt.tight_layout()
    plt.show()

from google.colab import drive
drive.mount('/content/drive/')

#install keras
! pip install -q keras

import keras

# import the necessary packages
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import VGG16,VGG19,DenseNet169,ResNet152
from tensorflow.keras.layers import AveragePooling2D,Conv2D
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Input
from tensorflow.keras.models import Model,load_model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from imutils import paths
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import *
import argparse  # I must understand this 
import cv2
import os
import sys
from glob import glob

dataset_path ='/content/drive/MyDrive/Colab Notebooks/base'

from random import shuffle, choice
from PIL import Image
import os
import numpy as np
import matplotlib as plt
import random
import glob
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import time
import pandas as pd
import cv2
import math
from random import randint

def load_rgb_data_cv(IMAGE_DIRECTORY,IMAGE_SIZE, shuffle=True):
    print("Loading images...")
    data = []
    labels=[]
    directories = next(os.walk(IMAGE_DIRECTORY))[1]

    for diretcory_name in directories:
        print("Loading {0}".format(diretcory_name))
        file_names = next(os.walk(os.path.join(IMAGE_DIRECTORY, diretcory_name)))[2]
        print("we will load [", len(file_names), "] files from [",diretcory_name,"] class ..." )
        for i in range(len(file_names)):
          image_name = file_names[i]
          image_path = os.path.join(IMAGE_DIRECTORY, diretcory_name, image_name)
          #print(image_path)
          label = diretcory_name

          img = cv2.imread(image_path)
          img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
          img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))


          #print(np.array(img).shape)
          data.append([np.array(img), label])

    if (shuffle):
      random.shuffle(data)
    training_images = np.array([i[0] for i in data])
    training_labels = np.array([i[1] for i in data])
    
    print("File loading completed.")

    return training_images, training_labels

data,labels=load_rgb_data_cv(dataset_path ,224,shuffle=True)
print(data.shape)
print(labels.shape)

def plot_sample_from_dataset(images, labels,rows=100, colums=100, width=8,height=8):

  plt.figure(figsize=(width,height))
  for i in range(rows*colums):
      plt.subplot(rows,colums,i+1)
      plt.xticks([])
      plt.yticks([])
      plt.grid(False)
      plt.imshow(images[i], cmap=plt.cm.binary)
      plt.xlabel(labels[i])
  plt.show()

#visualisation de la base : 25 images
plot_sample_from_dataset(data, labels,rows=5, colums=5, width=8,height=8)

display_dataset_folders(dataset_path)

# convert the data and labels to NumPy arrays

data ,labels = load_rgb_data_cv(dataset_path,224,shuffle=True)
data = np.array(data)
labels = np.array(labels)
# perform one-hot encoding on the labels
lb = LabelBinarizer()

labels = lb.fit_transform(labels)
labels = to_categorical(labels)

states=get_data_distribution(dataset_path,output_file='/content/drive/MyDrive/Colab Notebooks/Detection/19C_chest.csv')

histo=plot_dataset_distribution(states ,num_cols=5, width=10, height=5, histogram_bins = 10, histogram_range=[0, 2000], figure_padding=4)

# partition the data into training and testing splits using 80% of
# the data for training and the remaining 20% for testing
(trainX, testX, trainY, testY) = train_test_split(data, labels,
	test_size=0.20, stratify=labels, random_state=42)

display_dataset_shape(trainX,trainY)
display_dataset_shape(testX,testY)

#initialisation the training data augmentation
# initialize the training data augmentation object
trainAug = ImageDataGenerator(rotation_range=15,fill_mode="nearest")

##plot_sample_from_dataset(trainAug, labels,rows=5, colums=5, width=8,height=8)
from numpy import expand_dims
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.preprocessing.image import ImageDataGenerator
from matplotlib import pyplot
#data ,labels = load_rgb_data_cv(dataset_path,224,shuffle=True)
data = np.array(data)
# expand dimension to one sample
samples = expand_dims(data, 0)
# create image data augmentation generator
trainAug = ImageDataGenerator(rotation_range=15,fill_mode="nearest")
# prepare iterator
it = trainAug.flow(trainX, batch_size=1)
# generate samples and plot
for i in range(9):
	# define subplot
	pyplot.subplot(330 + 1 + i)
	# generate batch of images
	batch = it.next()
	# convert to unsigned integers for viewing
	image = batch[0].astype('uint8')
	# plot raw pixel data
	pyplot.imshow(image)
 
# show the figure
pyplot.show()

#develop th NN model 
#initialiser the learning rate , the epocks and the batch size
init_LR=1e-3
epocks=20
BS=8

#load the VGG16 network
vgg_model = VGG16(include_top=False ,weights='imagenet',input_tensor=Input(shape=(224,224,3)))

#transfert learning
headModel=vgg_model.output
headModel=AveragePooling2D(pool_size=(4,4))(headModel)
headModel=Flatten(name="flatten")(headModel)
headModel=Dense(64,activation="relu")(headModel)
headModel=Dropout(0.5)(headModel)
headModel=Dense(2,activation="softmax")(headModel)

#concatenetion de deux modeles
model=Model(inputs=vgg_model.input,outputs=headModel)

# juste train the headModel
#weights of the pre-trained model are not trained
for layer in vgg_model.layers:
  layer.trainable=False

#compiler le model
#compiler le model
opt = Adam(lr=init_LR,decay= init_LR/epocks)
model.compile(loss='binary_crossentropy', optimizer =opt ,metrics=["accuracy"])

#train the headModel  
H = model.fit_generator(trainAug.flow(trainX, trainY, batch_size=32),steps_per_epoch=len(trainX) // 32,validation_data=(testX, testY),validation_steps=len(testX) // 32,epochs=20)

model.save("covid-19_vgg16.h5")
print("saved")

#evaluation de modele
loss,test_acc=model.evaluate(testX,testY)
print(loss)
print(test_acc)

#load the model :model
model = tf.keras.models.load_model('covid-19_vgg16.h5')

#make prediction 
model.evaluate(testX,verbose=1)
prediction=model.predict(testX,verbose=1)

#prdict image by image 
indexe=3
image=testX[indexe]
display_image(testX,testY,indexe)

image =reshape_image_for_neural_network_input(image, IMAGE_SIZE=224)
predictImage=model.predict(image)
print('true label :',testY[indexe])
print('predict label:',predictImage)

predictIndex=np.argmax(predictImage,axis=1)
print('predicted label :',predictIndex)

predictProp=model.predict(testX,verbose=1)
predictInx=np.argmax(predictProp,axis=1)
print(classification_report(testY.argmax(axis=1),predictInx,target_names=lb.classes_))

#matrix de confusion
matrix=confusion_matrix(testY.argmax(axis=1),predictInx)
total=sum(sum(matrix))
acc = (matrix[0][0]+matrix[1][1])/(matrix[0][0]+matrix[0][1]+matrix[1][0]+matrix[1][1])
sensitivity=matrix[0][0]/(matrix[0][0]+matrix[0][1])
specificity=matrix[1][1]/(matrix[1][0]+matrix[1][1])
print('acc :',format(acc))
print(' sensitivity:',format(sensitivity))
print('specificity :',format(specificity))

#plot_loss_accuracy(H,21,'/content/drive/MyDrive/Colab Notebooks/base/plot.jpg')
plot_loss_accuracy(H, 20, output_file='/content/drive/MyDrive/Colab Notebooks/base/plot.jpg')

N=epocks
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, N), H.history["loss"], label="train_loss")
plt.plot(np.arange(0, N), H.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, N), H.history["accuracy"], label="train_acc")
plt.plot(np.arange(0, N), H.history["val_accuracy"], label="val_acc")
plt.title("Training Loss and Accuracy on COVID-19 Dataset")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend(loc="lower rigth")
plt.savefig('/content/drive/MyDrive/Colab Notebooks/base/figureCovid.jpg')

#deep learning from the scratch
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers,Sequential
model_from_scratch = Sequential()
model_from_scratch.add(Conv2D(32, 3, name='conv1', activation='relu',input_shape=(224, 224, 3)))
model_from_scratch.add(Conv2D(32, 3, name='conv2', activation='relu'))
model_from_scratch.add(layers.MaxPooling2D(2, name='pool1'))
model_from_scratch.add(Conv2D(64, 3, name='conv3', activation='relu'))
model_from_scratch.add(Conv2D(64, 3, name='conv4', activation='relu'))
model_from_scratch.add(layers.MaxPooling2D(4, name='pool2'))
model_from_scratch.add(Flatten())
model_from_scratch.add(Dense(64, name='fullyconnected', activation='relu'))
model_from_scratch.add(Dropout(0.5))
model_from_scratch.add(Dense(2, name='dense', activation='softmax'))
model_from_scratch.summary()

model_from_scratch.compile(loss='binary_crossentropy', optimizer =opt ,metrics=["accuracy"])
H = model_from_scratch.fit_generator(trainAug.flow(trainX, trainY, batch_size=32),steps_per_epoch=len(trainX) // 32,validation_data=(testX, testY),validation_steps=len(testX) // 32,epochs=20)
#evaluation de modele from the scratch
loss,test_acc=model_from_scratch.evaluate(testX,testY)
print(loss)
print(test_acc)

# VGG16 model using transfert learning
model.summary()

vgg19_model = VGG19(include_top=False ,weights='imagenet',input_tensor=Input(shape=(224,224,3)))

#transfert learning with vgg19
headModel19=vgg19_model.output
headModel19=AveragePooling2D(pool_size=(4,4))(headModel19)
headModel19=Flatten(name="flatten")(headModel19)
headModel19=Dense(64,activation="relu")(headModel19)
headModel19=Dropout(0.5)(headModel19)
headModel19=Dense(2,activation="softmax")(headModel19)

#concatenetion de deux modeles
model_vgg19=Model(inputs=vgg19_model.input,outputs=headModel19)

# juste train the headModel19
#weights of the pre-trained model are not trained
for layer in vgg19_model.layers:
  layer.trainable=False

#compiler le model
opt = Adam(lr=init_LR,decay= init_LR/epocks)
model_vgg19.compile(loss='binary_crossentropy', optimizer =opt ,metrics=["accuracy"])

#train the headModel  
H = model_vgg19.fit_generator(trainAug.flow(trainX, trainY, batch_size=32),steps_per_epoch=len(trainX) // 32,validation_data=(testX, testY),validation_steps=len(testX) // 32,epochs=20)

#evaluation de modele
loss,test_acc=model_vgg19.evaluate(testX,testY)
print(loss)
print(test_acc)

predictProp=model_vgg19.predict(testX,verbose=1)
predictInx=np.argmax(predictProp,axis=1)
print(classification_report(testY.argmax(axis=1),predictInx,target_names=lb.classes_))

matrix=confusion_matrix(testY.argmax(axis=1),predictInx)
total=sum(sum(matrix))
acc = (matrix[0][0]+matrix[1][1])/(matrix[0][0]+matrix[0][1]+matrix[1][0]+matrix[1][1])
sensitivity=matrix[0][0]/(matrix[0][0]+matrix[0][1])
specificity=matrix[1][1]/(matrix[1][0]+matrix[1][1])
print('acc :',format(acc))
print(' sensitivity:',format(sensitivity))
print('specificity :',format(specificity))

N=epocks
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, N), H.history["loss"], label="train_loss")
plt.plot(np.arange(0, N), H.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, N), H.history["accuracy"], label="train_acc")
plt.plot(np.arange(0, N), H.history["val_accuracy"], label="val_acc")
plt.title("Training Loss and Accuracy on COVID-19 Dataset")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend(loc="lower rigth")
plt.savefig('/content/drive/MyDrive/Colab Notebooks/base/figureCovid_with_VGG19.jpg')

#DenseNet169
dansenet169_model =DenseNet169( include_top=False, weights="imagenet",  input_tensor=Input(shape=(224,224,3)))

#transfert learning with DanseNet169
headModel169=dansenet169_model.output
headModel169=AveragePooling2D(pool_size=(4,4))(headModel169)
headModel169=Flatten(name="flatten")(headModel169)
headModel169=Dense(64,activation="relu")(headModel169)
headModel169=Dropout(0.5)(headModel169)
headModel169=Dense(2,activation="softmax")(headModel169)

#concatenetion de deux modeles
dansenet169=Model(inputs=dansenet169_model.input,outputs=headModel169)

# juste train the headModel169
#weights of the pre-trained model are not trained
for layer in dansenet169.layers:
  layer.trainable=False

#compiler le model
opt = Adam(lr=init_LR,decay= init_LR/epocks)
dansenet169.compile(loss='binary_crossentropy', optimizer =opt ,metrics=["accuracy"])

#train the headModel169  
H = dansenet169.fit_generator(trainAug.flow(trainX, trainY, batch_size=32),steps_per_epoch=len(trainX) // 32,validation_data=(testX, testY),validation_steps=len(testX) // 32,epochs=20)

#evaluation de modele
loss,test_acc=dansenet169.evaluate(testX,testY)
print(loss)
print(test_acc)

predictProp1=dansenet169.predict(testX,verbose=1)
predictInx1=np.argmax(predictProp1,axis=1)
print(classification_report(testY.argmax(axis=1),predictInx1,target_names=lb.classes_))

dansenet169.summary()

matrix=confusion_matrix(testY.argmax(axis=1),predictInx1)
total=sum(sum(matrix))
acc = (matrix[0][0]+matrix[1][1])/(matrix[0][0]+matrix[0][1]+matrix[1][0]+matrix[1][1])
sensitivity=matrix[0][0]/(matrix[0][0]+matrix[0][1])
specificity=matrix[1][1]/(matrix[1][0]+matrix[1][1])
print('acc :',format(acc))
print(' sensitivity:',format(sensitivity))
print('specificity :',format(specificity))

N=epocks
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, N), H.history["loss"], label="train_loss")
plt.plot(np.arange(0, N), H.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, N), H.history["accuracy"], label="train_acc")
plt.plot(np.arange(0, N), H.history["val_accuracy"], label="val_acc")
plt.title("Training Loss and Accuracy on COVID-19 Dataset")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend(loc="lower rigth")
plt.savefig('/content/drive/MyDrive/Colab Notebooks/base/figureCovid_with_VGG19.jpg')

#ResNet152
ResNet152_model=ResNet152(include_top=False, weights="imagenet", input_tensor=Input(shape=(224,224,3)))

#transfert learning with ResNet169
headModel152=ResNet152_model.output
headModel152=AveragePooling2D(pool_size=(4,4))(headModel152)
headModel152=Flatten(name="flatten")(headModel152)
headModel152=Dense(64,activation="relu")(headModel152)
headModel152=Dropout(0.5)(headModel152)
headModel152=Dense(2,activation="softmax")(headModel152)

#concatenetion de deux modeles
ResNet152=Model(inputs=ResNet152_model.input,outputs=headModel152)

# juste train the headModel169
#weights of the pre-trained model are not trained
for layer in ResNet152.layers:
  layer.trainable=False

#compiler le model
opt = Adam(lr=init_LR,decay= init_LR/epocks)
ResNet152.compile(loss='binary_crossentropy', optimizer =opt ,metrics=["accuracy"])

ResNet152.summary()

#train the headModel169  
H = ResNet152.fit_generator(trainAug.flow(trainX, trainY, batch_size=32),steps_per_epoch=len(trainX) // 32,validation_data=(testX, testY),validation_steps=len(testX) // 32,epochs=20)

#evaluation de modele
loss,test_acc=ResNet152.evaluate(testX,testY)
print(loss)
print(test_acc)

predictProp2=ResNet152.predict(testX,verbose=1)
predictInx2=np.argmax(predictProp2,axis=1)
print(classification_report(testY.argmax(axis=1),predictInx1,target_names=lb.classes_))

matrix=confusion_matrix(testY.argmax(axis=1),predictInx2)
total=sum(sum(matrix))
acc = (matrix[0][0]+matrix[1][1])/(matrix[0][0]+matrix[0][1]+matrix[1][0]+matrix[1][1])
sensitivity=matrix[0][0]/(matrix[0][0]+matrix[0][1])
specificity=matrix[1][1]/(matrix[1][0]+matrix[1][1])
print('acc :',format(acc))
print(' sensitivity:',format(sensitivity))
print('specificity :',format(specificity))

N=epocks
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, N), H.history["loss"], label="train_loss")
plt.plot(np.arange(0, N), H.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, N), H.history["accuracy"], label="train_acc")
plt.plot(np.arange(0, N), H.history["val_accuracy"], label="val_acc")
plt.title("Training Loss and Accuracy on COVID-19 Dataset")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend(loc="lower rigth")
plt.savefig('/content/drive/MyDrive/Colab Notebooks/base/figureCovid_with_VGG19.jpg')
